{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c4c1101-2542-43e3-bcd8-3b5db8560824",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e63c21-7169-43cf-869d-5e8982ca34e4",
   "metadata": {},
   "source": [
    "## For running method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fafb0124-e52b-4b8d-a023-156acf6711ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import eigh, block_diag, pinv\n",
    "from scipy.linalg import eig\n",
    "import torch\n",
    "import pymanopt\n",
    "from pymanopt.manifolds import Product, Stiefel\n",
    "from pymanopt import Problem\n",
    "from pymanopt.optimizers import ConjugateGradient\n",
    "import autograd\n",
    "from scipy.linalg import qr\n",
    "from numpy.linalg import qr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25692b7f-30bd-46bb-b90b-3df851d71bcc",
   "metadata": {},
   "source": [
    "## For loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d408e144-9d12-4be4-abc7-98249a5c4903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb683ad-7f5e-46ce-bd74-4ef978900b88",
   "metadata": {},
   "source": [
    "## For Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6735e67a-9886-4fd4-a457-318de432b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.linalg import eigh, block_diag, pinv\n",
    "import torch\n",
    "import pymanopt\n",
    "from pymanopt.manifolds import Product, Stiefel\n",
    "from pymanopt import Problem\n",
    "from pymanopt.optimizers import ConjugateGradient\n",
    "import autograd\n",
    "from scipy.stats import ortho_group\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95f7f3e-1fc8-468f-948f-9a5e794123c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30e405d-43cf-4143-826c-901473772261",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59faec5b-3a6b-42c7-8d9b-23e03e2cda60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def method1(X, Y1, Y2, d1, a, l):\n",
    "    \"\"\"\n",
    "    Compute the desired matrices and eigenvectors as described in the prompt.\n",
    "\n",
    "    Parameters:\n",
    "        X (np.ndarray): Data matrix (n x p)\n",
    "        Y1 (np.ndarray): Binary response vector of length n\n",
    "        Y2 (np.ndarray): Binary response vector of length n\n",
    "        d1 (int): Number of eigenvalues/vectors for the first block\n",
    "        d2 (int): Number of eigenvalues/vectors for the second block\n",
    "        a (float): hyperparameter for contrastive term\n",
    "        l (float): hyperparameter for PCA term\n",
    "    Returns:\n",
    "        tuple: V1, V2 matrices corresponding to the extracted eigenvectors\n",
    "    \"\"\"\n",
    "    # Total number of eigenvalues/vectors to extract\n",
    "    d = d1\n",
    "    # Check dimensions\n",
    "    n, p = X.shape\n",
    "    assert len(Y1) == n and len(Y2) == n, \"Response vectors must have length n\"\n",
    "    assert d < p, \"d must be less than the number of features p\"\n",
    "    #assert a >= 0, \"contrastive hyperparameter must be nonnegative\"\n",
    "    #assert l >= 0, \"PCA hyperparameter must be nonnegative\"\n",
    "    \n",
    "    # Compute sample covariance of X\n",
    "    Sigma = np.cov(X, rowvar=False)\n",
    "    \n",
    "    epsilon = 1e-5  # Small positive value\n",
    "    Sigma_inv = np.linalg.inv(Sigma + epsilon * np.eye(Sigma.shape[0]))\n",
    "\n",
    "    def compute_sigma_1(X, Y):\n",
    "        \"\"\"Compute Sigma_1 (or Sigma_2) as described.\"\"\"\n",
    "        p_1 = np.mean(Y)  # Proportion of slice Y = 1\n",
    "        m_1 = np.mean(X[Y == 1], axis=0)  # Mean for Y = 1\n",
    "        m_0 = np.mean(X[Y == 0], axis=0)  # Mean for Y = 0\n",
    "        \n",
    "        # Compute Sigma_1\n",
    "        Sigma_1 = p_1 * np.outer(m_1, m_1) + (1 - p_1) * np.outer(m_0, m_0)\n",
    "        return Sigma_1\n",
    "\n",
    "    # Compute Sigma_1, Sigma_2, and Sigma_12\n",
    "    Sigma_1 = compute_sigma_1(X, Y1)\n",
    "    Sigma_2 = compute_sigma_1(X, Y2)\n",
    "    \n",
    "    # Compute A_1, A_2, A_12\n",
    "    A_1 = Sigma @ Sigma_1 @ Sigma\n",
    "    A_2 = Sigma @ Sigma_2 @ Sigma\n",
    "    \n",
    "    # Compute matrix to find top eigenvectors of\n",
    "    M1 = Sigma_inv @ (a * A_1 - A_2) @ Sigma_inv + l * Sigma_inv\n",
    "    M2 = Sigma_inv @ Sigma_inv\n",
    "    \n",
    "    M1 = (M1 + M1.T)/2\n",
    "    M2 = (M2 + M2.T)/2\n",
    "    \n",
    "    #print(\"Symmetry check for M1:\", np.allclose(M1, M1.T))\n",
    "    #print(\"Symmetry check for M2:\", np.allclose(M2, M2.T))\n",
    "\n",
    "    \n",
    "    #M2_cond_number = np.linalg.cond(M2)\n",
    "    #print(\"M2 condition number = \", end=\"\")\n",
    "    #print(M2_cond_number)\n",
    "    \n",
    "    # Compute eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = eig(M1, M2)\n",
    "    \n",
    "    # Coerce eigenvalues and eigenvectors to real values\n",
    "    eigenvalues = np.real(eigenvalues)\n",
    "    eigenvectors = np.real(eigenvectors)\n",
    "    \n",
    "    # Sort eigenvalues and eigenvectors in descending order\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    eigenvectors = eigenvectors[:, sorted_indices]\n",
    "    \n",
    "    #print(\"all eigenvalues = \", end=\"\")\n",
    "    #print(eigenvalues)\n",
    "    \n",
    "    # Select the top eigenvectors (e.g., top d1 eigenvectors)\n",
    "    top_eigenvalues = eigenvalues[:d1]\n",
    "    top_eigenvectors = eigenvectors[:, :d1]\n",
    "    \n",
    "    #print(\"top eigenvalues = \", end=\"\")\n",
    "    #print(top_eigenvalues)\n",
    "    \n",
    "    #print(\"pct var = \", end=\"\")\n",
    "    #print(sum(top_eigenvalues) / sum(eigenvalues) * 100)\n",
    "    \n",
    "    # Rotate back\n",
    "    V = Sigma_inv @ top_eigenvectors\n",
    "    \n",
    "    # Perform QR decomposition to orthogonalize V\n",
    "    Q, R = np.linalg.qr(V)\n",
    "    \n",
    "    return np.real(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1784436-3447-4c46-8063-28f139d6db77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def method2(X, Y1, Y2, d1, d2, e, a, b, c, l):\n",
    "    \"\"\"\n",
    "    Compute the desired matrices and eigenvectors as described in the prompt.\n",
    "\n",
    "    Parameters:\n",
    "        X (np.ndarray): Data matrix (n x p)\n",
    "        Y1 (np.ndarray): Binary response vector of length n\n",
    "        Y2 (np.ndarray): Binary response vector of length n\n",
    "        d1 (int): Number of eigenvalues/vectors for the first block\n",
    "        d2 (int): Number of eigenvalues/vectors for the second block\n",
    "        a (float): hyperparameter for contrastive term (alpha)\n",
    "        b (float): hyperparameter for V2 (beta)\n",
    "        c (float): hyperparameter for V2 contrastive term (gamma)\n",
    "        l (float): hyperparameter for PCA term (lambda)\n",
    "    Returns:\n",
    "        tuple: V1, V2 matrices corresponding to the extracted eigenvectors\n",
    "    \"\"\"\n",
    "    # Total number of eigenvalues/vectors to extract\n",
    "    d = d1 + d2\n",
    "    # Check dimensions\n",
    "    n, p = X.shape\n",
    "    assert len(Y1) == n and len(Y2) == n, \"Response vectors must have length n\"\n",
    "    assert d < p, \"d must be less than the number of features p\"\n",
    "    assert a >= 0, \"contrastive hyperparameter must be nonnegative\"\n",
    "    assert b >= 0, \"hyperparameter for V2 must be nonnegative\"\n",
    "    assert c >= 0, \"hyperparameter for V2 contrastive term must be nonnegative\"\n",
    "    assert l >= 0, \"PCA hyperparameter must be nonnegative\"\n",
    "    assert e >= 0, \"first hyperparameter must be nonnegative\"\n",
    "    assert max(a, b, c, l, e) >= 0, \"at least one hyperparameter must be positive\"\n",
    "    \n",
    "    # Compute sample covariance of X\n",
    "    Sigma = np.cov(X, rowvar=False)\n",
    "    #Sigma_inv = np.linalg.inv(Sigma)\n",
    "    B = Sigma @ Sigma\n",
    "\n",
    "    def compute_sigma_1(X, Y):\n",
    "        \"\"\"Compute Sigma_1 (or Sigma_2) as described.\"\"\"\n",
    "        p_1 = np.mean(Y)  # Proportion of slice Y = 1\n",
    "        m_1 = np.mean(X[Y == 1], axis=0)  # Mean for Y = 1\n",
    "        m_0 = np.mean(X[Y == 0], axis=0)  # Mean for Y = 0\n",
    "        \n",
    "        # Compute Sigma_1\n",
    "        Sigma_1 = p_1 * np.outer(m_1, m_1) + (1 - p_1) * np.outer(m_0, m_0)\n",
    "        return Sigma_1\n",
    "\n",
    "    # Compute Sigma_1, Sigma_2, and Sigma_12\n",
    "    Sigma_1 = compute_sigma_1(X, Y1)\n",
    "    Sigma_2 = compute_sigma_1(X, Y2)\n",
    "    \n",
    "    # Compute A_1, A_2, A_12\n",
    "    A_1 = Sigma @ Sigma_1 @ Sigma\n",
    "    A_2 = Sigma @ Sigma_2 @ Sigma\n",
    "    \n",
    "    # V1\n",
    "    # Compute matrix to find top eigenvectors of\n",
    "    M1 = Sigma @ (A_1 - a * A_2) @ Sigma\n",
    "    \n",
    "    # Compute eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors1 = np.linalg.eig(M1)\n",
    "    \n",
    "    # Sort eigenvalues and eigenvectors in descending order\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    eigenvectors1 = eigenvectors1[:, sorted_indices]\n",
    "    \n",
    "    # Select the top eigenvectors (e.g., top d1 eigenvectors)\n",
    "    top_eigenvectors1 = eigenvectors1[:, :d1]\n",
    "    \n",
    "    # Rotate back\n",
    "    V1 = np.linalg.solve(Sigma, top_eigenvectors1)\n",
    "    \n",
    "    # Perform QR decomposition to orthogonalize V\n",
    "    Q1, R = np.linalg.qr(V1)\n",
    "    \n",
    "    \n",
    "    # V2\n",
    "    # Compute matrix to find top eigenvectors of\n",
    "    M2 = Sigma @ (c * A_2 - b * A_1) @ Sigma\n",
    "    \n",
    "    # Compute eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors2 = np.linalg.eig(M2)\n",
    "    \n",
    "    # Sort eigenvalues and eigenvectors in descending order\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    eigenvectors2 = eigenvectors2[:, sorted_indices]\n",
    "    \n",
    "    # Select the top eigenvectors (e.g., top d1 eigenvectors)\n",
    "    top_eigenvectors2 = eigenvectors2[:, :d1]\n",
    "    \n",
    "    # Rotate back\n",
    "    V2 = np.linalg.solve(Sigma, top_eigenvectors2)\n",
    "    \n",
    "    # Perform QR decomposition to orthogonalize V\n",
    "    Q2, R = np.linalg.qr(V2)\n",
    "    \n",
    "    if l == 0:\n",
    "        return np.real(Q1), np.real(Q2)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Convert necessary matrices to PyTorch tensors\n",
    "        Sigma_cubed = torch.tensor(Sigma @ B, dtype=torch.float64)\n",
    "        A_1_torch = torch.tensor(A_1, dtype=torch.float64)\n",
    "        A_2_torch = torch.tensor(A_2, dtype=torch.float64)\n",
    "        B_torch = torch.tensor(B, dtype=torch.float64)\n",
    "        \n",
    "        # Define the product manifold\n",
    "        manifold = pymanopt.manifolds.Product([\n",
    "            pymanopt.manifolds.Stiefel(p, d1),\n",
    "            pymanopt.manifolds.Stiefel(p, d2)\n",
    "        ])\n",
    "\n",
    "        \n",
    "        @pymanopt.function.pytorch(manifold)\n",
    "        def cost(V_1, V_2):\n",
    "            V_combined = torch.cat([V_1, V_2], dim=1)\n",
    "            part1 = -torch.trace(V_1.T @ A_1_torch @ V_1)\n",
    "            part2 = -torch.trace(V_2.T @ A_2_torch @ V_2)\n",
    "            VDV = V_combined.T @ B_torch @ V_combined\n",
    "            VDV_inv = torch.linalg.pinv(VDV)\n",
    "            part3 = -l * torch.trace(V_combined.T @ Sigma_cubed @ V_combined @ VDV_inv)\n",
    "            return part1 + part2 + part3\n",
    "\n",
    "        # Create the optimization problem\n",
    "        problem = Problem(manifold=manifold, cost=cost)\n",
    "\n",
    "        # Use a conjugate gradient optimizer\n",
    "        optimizer = ConjugateGradient()\n",
    "        result = optimizer.run(problem)\n",
    "\n",
    "        # Extract optimized V_1 and V_2\n",
    "        V_1_opt, V_2_opt = result.point\n",
    "        \n",
    "        # Rotate back\n",
    "        V1 = np.linalg.solve(Sigma, V_1_opt)\n",
    "        V2 = np.linalg.solve(Sigma, V_2_opt)\n",
    "        \n",
    "        # Perform QR decomposition to orthogonalize V\n",
    "        Q1, R = np.linalg.qr(V1)\n",
    "        Q2, R = np.linalg.qr(V2)\n",
    "        \n",
    "        return Q1, Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2daa3991-52fe-42d2-bc1b-44727b87349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairPCA(X, Y, d, m):\n",
    "    # Compute the covariance matrix\n",
    "    Sigma = np.cov(X, rowvar=False)\n",
    "    \n",
    "    # Split the data into two groups based on Y\n",
    "    X0 = X[Y == 0]\n",
    "    X1 = X[Y == 1]\n",
    "    \n",
    "    # Compute mean vectors\n",
    "    mu0 = np.mean(X0, axis=0)\n",
    "    mu1 = np.mean(X1, axis=0)\n",
    "    \n",
    "    # Compute covariance matrices\n",
    "    Sigma0 = np.cov(X0, rowvar=False)\n",
    "    Sigma1 = np.cov(X1, rowvar=False)\n",
    "    \n",
    "    # Compute the second moment difference\n",
    "    S = Sigma1 + np.outer(mu1, mu1) - Sigma0 - np.outer(mu0, mu0)\n",
    "    \n",
    "    # Compute eigenvalues and eigenvectors of Sigma_diff\n",
    "    eigvals, eigvecs = np.linalg.eigh(S)\n",
    "    \n",
    "    # Sort eigenvectors by absolute eigenvalues in descending order\n",
    "    sorted_indices = np.argsort(-np.abs(eigvals))\n",
    "    top_m_eigvecs = eigvecs[:, sorted_indices[:m]]\n",
    "    \n",
    "    # Include mu0 - mu1 in the subspace\n",
    "    #mu_diff = (mu0 - mu1).reshape(-1, 1)\n",
    "    #combined_basis = np.hstack([top_m_eigvecs, mu_diff])\n",
    "    \n",
    "    # Perform QR decomposition to obtain an orthonormal basis\n",
    "    Q, _ = np.linalg.qr(top_m_eigvecs, mode='reduced')\n",
    "    I = np.eye(X.shape[1])  # Identity matrix of size p x p\n",
    "    Pi = I - Q @ Q.T  # Project onto orthogonal complement\n",
    "    \n",
    "    # Compute Pi Sigma Pi\n",
    "    transformed_Sigma = Pi @ Sigma @ Pi\n",
    "    \n",
    "    # Compute the top d eigenvectors of Pi Sigma Pi\n",
    "    eigvals_final, eigvecs_final = np.linalg.eigh(transformed_Sigma)\n",
    "    sorted_indices_final = np.argsort(-eigvals_final)\n",
    "    top_d_eigvecs = eigvecs_final[:, sorted_indices_final[:d]]\n",
    "    \n",
    "    return top_d_eigvecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c4f1ae-5524-4e73-8ca1-4341ebfec197",
   "metadata": {},
   "source": [
    "### SIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c959b31-6277-4a67-898e-00f853af9a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIR(X, Y, d):\n",
    "    # Compute sample covariance of X\n",
    "    Sigma = np.cov(X, rowvar=False)\n",
    "    \n",
    "    epsilon = 1e-5  # Small positive value for regularization\n",
    "    U, S, Vt = np.linalg.svd(Sigma)\n",
    "    Sigma_inv = Vt.T @ np.diag(1 / (S + epsilon)) @ U.T  # More stable inversion\n",
    "\n",
    "    def compute_sigma_1(X, Y):\n",
    "        \"\"\"Compute Sigma_1 (Centered Covariance).\"\"\"\n",
    "        unique_classes = np.unique(Y)\n",
    "        overall_mean = np.mean(X, axis=0)\n",
    "        Sigma_1 = np.zeros((X.shape[1], X.shape[1]))\n",
    "        \n",
    "        for y in unique_classes:\n",
    "            p_h = np.mean(Y == y)\n",
    "            m_h = np.mean(X[Y == y], axis=0)\n",
    "            Sigma_1 += p_h * np.outer(m_h - overall_mean, m_h - overall_mean)\n",
    "        \n",
    "        return Sigma_1\n",
    "\n",
    "    # Compute Sigma_1\n",
    "    Sigma_1 = compute_sigma_1(X, Y)\n",
    "    \n",
    "    # Compute transformation matrix\n",
    "    M = Sigma_inv @ Sigma_1\n",
    "    \n",
    "    # Ensure symmetry for numerical stability\n",
    "    M = (M + M.T) / 2\n",
    "\n",
    "    # Compute eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = eig(M)\n",
    "    \n",
    "    # Sort eigenvalues in descending order\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvectors = np.real(eigenvectors[:, sorted_indices])\n",
    "\n",
    "    # Select the top d eigenvectors\n",
    "    V = eigenvectors[:, :d]\n",
    "    \n",
    "    return V  # No need for QR decomposition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361d90f9-fbb2-4d02-9c34-084fd6b2c9b1",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd369429-4624-45cb-a060-b39f6ffa3db2",
   "metadata": {},
   "source": [
    "### Maximum Mean Discrepancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c43a0360-0b28-4577-85ae-c409c308e277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmd(X1, X2, sigma, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Compute the Maximum Mean Discrepancy (MMD) between two datasets X1 and X2 with memory efficiency.\n",
    "    \"\"\"\n",
    "    def rbf_batch(X1, X2, sigma):\n",
    "        \"\"\"\n",
    "        Compute the Radial Basis Function (RBF) kernel between two datasets in batches.\n",
    "        \"\"\"\n",
    "        result = 0.0\n",
    "        for i in range(0, X1.shape[0], batch_size):\n",
    "            X1_batch = X1[i:i + batch_size]\n",
    "            distances = cdist(X1_batch, X2, 'sqeuclidean')\n",
    "            result += np.sum(np.exp(-distances / (2 * sigma**2)))\n",
    "        return result\n",
    "\n",
    "    m = X1.shape[0]\n",
    "    n = X2.shape[0]\n",
    "\n",
    "    # Term 1: Intra-set distances for X1\n",
    "    term1 = (1 / m**2) * rbf_batch(X1, X1, sigma)\n",
    "\n",
    "    # Term 2: Intra-set distances for X2\n",
    "    term2 = (1 / n**2) * rbf_batch(X2, X2, sigma)\n",
    "\n",
    "    # Term 3: Inter-set distances between X1 and X2\n",
    "    term3 = (2 / (m * n)) * rbf_batch(X1, X2, sigma)\n",
    "\n",
    "    d = term1 + term2 - term3\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698208ed-7764-42fb-91f0-278272564a88",
   "metadata": {},
   "source": [
    "### Percent Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92678b31-ff1a-48a6-8af1-c2fb7b369125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_var(X, V):\n",
    "    \"\"\"\n",
    "    Compute the % of variance retained by the projection in the reduced dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute sample covariance of X\n",
    "    Sigma = np.cov(X, rowvar=False)\n",
    "    \n",
    "    return 100 * np.trace(V.T @ Sigma @ V) / np.trace(Sigma) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2036d0b6-0205-45d3-9ea3-e29690eb3602",
   "metadata": {},
   "source": [
    "### Percent Accuracy (for Y1), Delta DP (predictive power of Y1 between the Y2 Groups) and F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35bd10f5-d169-46a4-a217-9f9d2883486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_svm_metrics(X_reduced_data, Y1_labels, Y2_groups, kernel='rbf', C=1.0, gamma='scale', cv=5):\n",
    "    \"\"\"\n",
    "    Compute cross-validated Accuracy (%Acc), F1 score (%F1), and Demographic Parity difference (%∆DP) for a kernel SVM.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_reduced_data: np.ndarray, the reduced data matrix (n_samples, n_features)\n",
    "    - Y1_labels: np.ndarray, the target labels (n_samples,)\n",
    "    - Y2_groups: np.ndarray, the demographic group labels (n_samples,)\n",
    "    - kernel: str, the kernel type to use for SVM ('linear', 'rbf', etc.)\n",
    "    - C: float, regularization parameter for SVM\n",
    "    - gamma: str or float, kernel coefficient for 'rbf', 'poly', and 'sigmoid'\n",
    "    - cv: int, the number of cross-validation folds\n",
    "\n",
    "    Returns:\n",
    "    - mean_acc, std_acc: Mean and standard deviation of accuracy (%Acc)\n",
    "    - mean_f1, std_f1: Mean and standard deviation of F1 score (%F1)\n",
    "    - mean_dp, std_dp: Mean and standard deviation of demographic parity difference (%∆DP)\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    svm = SVC(kernel=kernel, C=C, gamma=gamma)\n",
    "    \n",
    "    acc_scores = []\n",
    "    f1_scores = []\n",
    "    dp_differences = []\n",
    "    \n",
    "    for train_index, test_index in skf.split(X_reduced_data, Y1_labels):\n",
    "        X_train, X_test = X_reduced_data[train_index], X_reduced_data[test_index]\n",
    "        y_train, y_test = Y1_labels[train_index], Y1_labels[test_index]\n",
    "        g_test = Y2_groups[test_index]\n",
    "        \n",
    "        svm.fit(X_train, y_train)\n",
    "        y_pred = svm.predict(X_test)\n",
    "        \n",
    "        # Compute Accuracy and F1 Score\n",
    "        acc_scores.append(accuracy_score(y_test, y_pred) * 100)\n",
    "        f1_scores.append(f1_score(y_test, y_pred) * 100)\n",
    "        \n",
    "        # Compute Demographic Parity difference\n",
    "        positive_rates = {}\n",
    "        for group in np.unique(g_test):\n",
    "            group_indices = np.where(g_test == group)[0]\n",
    "            positive_rate = np.mean(y_pred[group_indices] == 1)\n",
    "            positive_rates[group] = positive_rate\n",
    "        \n",
    "        group_rates = list(positive_rates.values())\n",
    "        if len(group_rates) == 2:  # Ensure binary groups\n",
    "            dp_difference = abs(group_rates[0] - group_rates[1])\n",
    "            dp_differences.append(dp_difference * 100)\n",
    "        \n",
    "    return (\n",
    "        np.mean(acc_scores), np.std(acc_scores),\n",
    "        np.mean(f1_scores), np.std(f1_scores),\n",
    "        np.mean(dp_differences), np.std(dp_differences)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e1ea67-f34e-41b0-b55e-06522b624b86",
   "metadata": {},
   "source": [
    "### Demographic Parity (predictive power of Y1 between the Y2 groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fbb2d56-02e2-40b7-a6ee-6d3a46d3b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_delta_demographic_parity(X_reduced_data, Y1_labels, Y2_groups, kernel='rbf', C=1.0, gamma='scale', cv=5):\n",
    "    \"\"\"\n",
    "    Compute %∆DP (Demographic Parity) for a kernel SVM.\n",
    "\n",
    "    Parameters:\n",
    "    - X_reduced_data: np.ndarray, the reduced data matrix (n_samples, n_features)\n",
    "    - Y2_labels: np.ndarray, the target labels (n_samples,)\n",
    "    - groups: np.ndarray, the demographic group labels (n_samples,)\n",
    "    - kernel: str, the kernel type to use for SVM ('linear', 'rbf', etc.)\n",
    "    - C: float, regularization parameter for SVM\n",
    "    - gamma: str or float, kernel coefficient for 'rbf', 'poly', and 'sigmoid'\n",
    "    - cv: int, the number of cross-validation folds\n",
    "\n",
    "    Returns:\n",
    "    - mean_dp: float, mean demographic parity (%∆DP) across folds\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    svm = SVC(kernel=kernel, C=C, gamma=gamma)\n",
    "\n",
    "    dp_differences = []\n",
    "\n",
    "    for train_index, test_index in skf.split(X_reduced_data, Y1_labels):\n",
    "        X_train, X_test = X_reduced_data[train_index], X_reduced_data[test_index]\n",
    "        y_train, y_test = Y1_labels[train_index], Y1_labels[test_index]\n",
    "        g_test = Y2_groups[test_index]\n",
    "\n",
    "        svm.fit(X_train, y_train)\n",
    "        y_pred = svm.predict(X_test)\n",
    "\n",
    "        # Compute positive prediction rates per group\n",
    "        positive_rates = {}\n",
    "        for group in np.unique(g_test):\n",
    "            group_indices = np.where(g_test == group)[0]\n",
    "            positive_rate = np.mean(y_pred[group_indices] == 1)\n",
    "            positive_rates[group] = positive_rate\n",
    "\n",
    "        # Compute absolute differences between groups\n",
    "        group_rates = list(positive_rates.values())\n",
    "        dp_difference = abs(group_rates[0] - group_rates[1])  # Adjust for two-group assumption\n",
    "        dp_differences.append(dp_difference)\n",
    "\n",
    "    mean_dp = np.mean(dp_differences) * 100  # Convert to percentage\n",
    "    sd_dp = np.std(dp_differences) * 100 # Convert to percentage\n",
    "    return mean_dp, sd_dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca839ce8-e9fe-4647-b3ce-56741f8a04e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_delta_demographic_parity_logistic(X_reduced_data, Y1_labels, Y2_groups, C=1.0, cv=5):\n",
    "    \"\"\"\n",
    "    Compute %∆DP (Demographic Parity) for a Logistic Regression classifier.\n",
    "\n",
    "    Parameters:\n",
    "    - X_reduced_data: np.ndarray, the reduced data matrix (n_samples, n_features)\n",
    "    - Y1_labels: np.ndarray, the target labels (n_samples,)\n",
    "    - Y2_groups: np.ndarray, the demographic group labels (n_samples,)\n",
    "    - C: float, regularization parameter for Logistic Regression (default 1.0)\n",
    "    - cv: int, the number of cross-validation folds (default 5)\n",
    "\n",
    "    Returns:\n",
    "    - mean_dp: float, mean demographic parity (%∆DP) across folds\n",
    "    - sd_dp: float, standard deviation of demographic parity across folds\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    logistic_model = LogisticRegression(C=C, solver=\"liblinear\", random_state=42)  # Use liblinear for small datasets\n",
    "\n",
    "    dp_differences = []\n",
    "\n",
    "    for train_index, test_index in skf.split(X_reduced_data, Y1_labels):\n",
    "        X_train, X_test = X_reduced_data[train_index], X_reduced_data[test_index]\n",
    "        y_train, y_test = Y1_labels[train_index], Y1_labels[test_index]\n",
    "        g_test = Y2_groups[test_index]\n",
    "\n",
    "        logistic_model.fit(X_train, y_train)\n",
    "        y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "        # Compute positive prediction rates per group\n",
    "        positive_rates = {}\n",
    "        for group in np.unique(g_test):\n",
    "            group_indices = np.where(g_test == group)[0]\n",
    "            positive_rate = np.mean(y_pred[group_indices] == 1)\n",
    "            positive_rates[group] = positive_rate\n",
    "\n",
    "        # Compute absolute differences between groups\n",
    "        group_rates = list(positive_rates.values())\n",
    "        dp_difference = abs(group_rates[0] - group_rates[1])  # Adjust for two-group assumption\n",
    "        dp_differences.append(dp_difference)\n",
    "\n",
    "    mean_dp = np.mean(dp_differences) * 100  # Convert to percentage\n",
    "    sd_dp = np.std(dp_differences) * 100  # Convert to percentage\n",
    "    return mean_dp, sd_dp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719cb783-a6e2-4fea-a051-ad4a9514c2c8",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de3e2c90-cf8b-4a53-a910-458ed7dd7844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reduced_X(X, V, W, method_name=\"DR\", W_name=\"Group\", group_level_0=\"Class 0\", group_level_1=\"Class 1\", caption=None):\n",
    "    \"\"\"\n",
    "    Projects X onto the factor loading matrix V and plots the reduced data, \n",
    "    colored by the binary variable W.\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.ndarray): The original data matrix of shape (n, p).\n",
    "    - V (np.ndarray): The factor loading matrix of shape (p, 2).\n",
    "    - W (np.ndarray): A binary variable of shape (n,) used for coloring.\n",
    "    - method_name (str): Name of the dimension reduction method (e.g., \"SIRFairRe\").\n",
    "    - W_name (str): Name of the binary variable for labeling.\n",
    "    - group_level_0 (str): Label for group 0.\n",
    "    - group_level_1 (str): Label for group 1.\n",
    "    - caption (str, optional): Caption to display below the plot.\n",
    "\n",
    "    Returns:\n",
    "    - None (displays a scatter plot).\n",
    "    \"\"\"\n",
    "    # Compute the reduced representation (n x 2)\n",
    "    X_reduced = X @ V  # Matrix multiplication\n",
    "    cmap = mcolors.ListedColormap(['red', 'blue'])  # 0 → Red, 1 → Blue\n",
    "\n",
    "    # Create the scatter plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=W, cmap=cmap, edgecolors=\"k\", alpha=0.7)\n",
    "\n",
    "    # Create a custom legend\n",
    "    handles = [\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=8, label=group_level_0),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=8, label=group_level_1)\n",
    "    ]\n",
    "    plt.legend(handles=handles, title=W_name, fontsize=16)\n",
    "\n",
    "    # Label axes dynamically based on method name\n",
    "    plt.xlabel(f\"{method_name} 1\", fontsize=20)\n",
    "    plt.ylabel(f\"{method_name} 2\", fontsize=20)\n",
    "    plt.title(f\"{method_name} labeled by {W_name}\", fontsize=20)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    \n",
    "    # Add caption if provided\n",
    "    if caption:\n",
    "        plt.figtext(0.5, -0.05, caption, wrap=True, horizontalalignment='center', fontsize=20)\n",
    "\n",
    "    # Save the plot before displaying\n",
    "    filename = f\"GERMAN_Reduced_Dim_{method_name.replace('%', '').replace(' ', '_')}_{W_name.replace(' ', '_')}.png\"\n",
    "    plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cac273-6986-47e8-8ffc-c3ad5e329204",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e90f0f-f6e7-4197-84bc-0efa941416cf",
   "metadata": {},
   "source": [
    "## GERMAN UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41a02388-4044-462c-ab96-38ae47dfdd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import GermanDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5db4bb1-3dee-40c2-afbc-d02915a376d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "german = GermanDataset(\n",
    "    protected_attribute_names=['age'],\n",
    "    privileged_classes=[lambda x: x >= 25],      # age >=25 is considered privileged\n",
    "    # features_to_drop=['sex', 'personal_status'],\n",
    "    categorical_features=['status', 'credit_history', 'purpose',\n",
    "                     'savings', 'employment', 'other_debtors', 'property',\n",
    "                     'installment_plans', 'housing', 'skill_level', 'telephone',\n",
    "                     'foreign_worker', 'sex'],\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a366e979-dd4e-4248-9fd8-e969f72ac8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas DataFrame\n",
    "df, _ = german.convert_to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921cc20c-e796-43af-9232-42db7b7bfa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb05045d-34fc-4744-b1e4-ee03d7864927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (drop 'label' and 'age' while keeping 59 features)\n",
    "X = df.drop(columns=['credit', 'age'])\n",
    "\n",
    "# Extract response labels\n",
    "Y = (df['credit'] == 1).astype(int)  # 1 for good credit, 0 for bad credit\n",
    "Z = (df['age'] == 1).astype(int)    # 1 for age >= 25, 0 for age < 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4574f337-8aa5-44da-85f2-334fd2781db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2537bcac-602b-48d6-974d-d842ea9b094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fef19a2-937d-48b6-a8db-ce86d281ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.corr(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b7fe20-5eed-4b98-a79a-a47fb908ea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "379060cd-455e-4978-86fd-8a8cbca91643",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = X.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89685336-c93d-43c5-b4c5-d362d8eff19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify highly correlated pairs (absolute correlation > 0.9)\n",
    "high_corr_pairs = [(i, j, corr_matrix.loc[i, j]) \n",
    "                   for i in corr_matrix.columns \n",
    "                   for j in corr_matrix.columns \n",
    "                   if i != j and abs(corr_matrix.loc[i, j]) > 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7601bf-5c89-49ca-96a7-9ecb1314a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_corr_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94430560-f5e4-42c8-a41e-96e6adf4e727",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(columns=[\"telephone=A192\", \"foreign_worker=A202\", \"sex=male\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e7125d-7402-4b09-bc8a-5b735e6a67b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67467b2a-ff5f-4f4a-8138-1fbc4856134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize only the continuous features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4251a75a-6b35-4e5b-80c2-6303fd153075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume X is your data matrix (n_samples x n_features)\n",
    "# Standardize X before computing PCA\n",
    "X_centered = X - np.mean(X, axis=0)  # Centering the data (zero mean)\n",
    "\n",
    "# Compute the covariance matrix\n",
    "cov_matrix = np.cov(X_centered, rowvar=False)\n",
    "\n",
    "# Compute eigenvalues (variances along principal components)\n",
    "eigenvalues, _ = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "# Sort eigenvalues in descending order\n",
    "eigenvalues = np.sort(eigenvalues)[::-1]\n",
    "\n",
    "# Compute the explained variance ratio\n",
    "explained_variance_ratio = eigenvalues / np.sum(eigenvalues)\n",
    "\n",
    "# Cumulative variance\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Plot Scree Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(eigenvalues) + 1), explained_variance_ratio, marker='o', linestyle='--', label=\"Explained Variance\")\n",
    "plt.plot(range(1, len(eigenvalues) + 1), cumulative_variance, marker='s', linestyle='-', label=\"Cumulative Variance\")\n",
    "\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"Variance Explained\")\n",
    "plt.title(\"Scree Plot\")\n",
    "plt.axhline(y=0.50, color='r', linestyle=\"--\", label=\"50% Variance Threshold\")  # Highlight 90% variance line\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97440598-0aca-4ed0-bcee-8291d05eba57",
   "metadata": {},
   "source": [
    "### Preprocess so that X0 and X1 centered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "705b54f9-1b13-4eb6-937c-3e25b1b69510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X based on S\n",
    "X_0, X_1 = X[Z == 0], X[Z == 1]\n",
    "    \n",
    "# Compute group-wise means\n",
    "mu_0, mu_1 = X_0.mean(axis=0), X_1.mean(axis=0)\n",
    "    \n",
    "# Center each group separately\n",
    "X_0_centered = X_0 - mu_0\n",
    "X_1_centered = X_1 - mu_1\n",
    "\n",
    "# Reassemble into original order\n",
    "X_centered = np.empty_like(X)\n",
    "X_centered[Z == 0] = X_0_centered\n",
    "X_centered[Z == 1] = X_1_centered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf21584-7ad6-4ecf-b867-217770f9c82d",
   "metadata": {},
   "source": [
    "# Parameter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "449eba07-779e-4f5c-b3f7-e634440dd2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [-2, -1, -0.5, -0.25, -0.1, 0, 0.1, 0.25, 0.5, 1, 2, 5, 10]\n",
    "parameters_list = [[x, y] for x in values for y in values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9772ba39-ae53-4b62-b985-38e371bc00fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2cf858-f4c4-4ff5-a08b-4056cd364986",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_list[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f741e70-fb1f-47a3-8938-039709ae92cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_list[166]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a8f26-fa4a-45ab-bccd-fc1f0608dff1",
   "metadata": {},
   "source": [
    "# Compute All Loadings of Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27013002-b73e-4736-9f2a-5fb0dbf5af82",
   "metadata": {},
   "source": [
    "## Option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "902a123c-4eca-48f1-936c-7ed3a810c480",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vs = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e7c3ea0-27a8-4f14-b1f1-d20e9f4895d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for setting in parameters_list:\n",
    "    al = tuple(setting)\n",
    "    a, l, = al\n",
    "    V = method1(X, Y, Z, 2, a, l)\n",
    "    Vs[al] = V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6e8da4-18eb-4096-b442-18ba2375acb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22606d50-0461-49fb-863b-bf8efc0c77f8",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fcd79de5-e750-4446-9d1b-1aca741cd619",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma = np.cov(X, rowvar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f1df22e-ff8f-470f-9526-7afe324e53b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = eigh(Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "370fba73-319f-4574-89b0-a8a7c8497dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_PCA = eigenvectors[:, -2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd73082e-b772-4c9d-8824-1862ea09769e",
   "metadata": {},
   "source": [
    "## SIR(Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d85f3b9-7844-4dc4-a6e3-11efe1e98724",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_SIRY1 = SIR(X, Y, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafea7c7-ca60-4a7e-be37-fb8b3fdebc49",
   "metadata": {},
   "source": [
    "## -SIR(Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f97a695-4f81-498b-8ed1-ec79070ef900",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_negSIRY2 = method1(X, Y, Z, 2, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3057c2c7-1666-45fb-889c-fc7174e147a3",
   "metadata": {},
   "source": [
    "## FairPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04bea5f1-f197-4c25-9b43-efe360cad79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vs_fair = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9fa181a2-a854-4687-91db-eb9815dfe1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = range(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef29a6f8-f44e-4927-af36-6dd8e472afd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)  # Convert X to NumPy array\n",
    "Y = np.array(Y).flatten()  # Ensure Y is a 1D NumPy array\n",
    "Z = np.array(Z).flatten()  # Ensure Z is a 1D NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3acb33c2-bf4a-4e5c-aed5-df6ac145658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in ms:\n",
    "    V = fairPCA(X, Z, 2, m)\n",
    "    Vs_fair[m] = V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2d902f-50ac-4802-9e70-3ef3e817523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a09951d-1e53-4ca0-8aba-41a082610ae8",
   "metadata": {},
   "source": [
    "# Perform Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "34a8025d-78a0-4f45-a799-d2590b4f0d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits(X, Y1_labels, cv=5, random_state=42):\n",
    "    \"\"\"Precompute and store cross-validation splits.\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=random_state)\n",
    "    splits = [(train_idx, test_idx) for train_idx, test_idx in skf.split(X, Y1_labels)]\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e5e55353-f4c9-446c-9427-db6706d4a039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sigma(X_reduced, sample_frac=0.1):\n",
    "    \"\"\"Compute sigma efficiently using a subset of the data.\"\"\"\n",
    "    n_samples = max(10, int(sample_frac * X_reduced.shape[0]))  # At least 10 samples\n",
    "    X_sample = X_reduced[np.random.choice(X_reduced.shape[0], n_samples, replace=False)]\n",
    "    pairwise_sq_distances = pdist(X_sample, 'sqeuclidean')\n",
    "    return np.sqrt(np.median(pairwise_sq_distances) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1162c28d-2281-4f2d-8d7e-44ea42c281f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define process_factor_loading globally and explicitly pass parameters\n",
    "def process_factor_loading(V, i, X, Y1_labels, Y2_groups, sample_frac, kernel, C, gamma, cv, splits):\n",
    "    \"\"\"Compute all metrics for a given factor loading.\"\"\"\n",
    "    print(f\"Processing factor loading {i}\")\n",
    "    \n",
    "    X_reduced = X @ V  # Project data\n",
    "\n",
    "    # Compute MMD sigma\n",
    "    sigma = compute_sigma(X_reduced, sample_frac=sample_frac)\n",
    "\n",
    "    # Compute metrics\n",
    "    mmd_score = mmd(X_reduced[Y1_labels == 0], X_reduced[Y1_labels == 1], sigma)\n",
    "    var_pct = pct_var(X, V)\n",
    "\n",
    "    # Compute accuracy and demographic parity\n",
    "    #acc_mean, acc_std = pct_accuracy(X_reduced, Y1_labels, kernel=kernel, C=C, gamma=gamma, cv=cv)\n",
    "    #dp_mean, dp_std = pct_delta_demographic_parity(X_reduced, Y1_labels, Y2_groups, kernel=kernel, C=C, gamma=gamma, cv=cv)\n",
    "    acc_mean, acc_std, f1_mean, f1_std, dp_mean, dp_std = compute_svm_metrics(X_reduced, Y1_labels, Y2_groups, kernel='rbf', C=1.0, gamma='scale', cv=5)\n",
    "    #dp_mean_logistic, dp_std_logistic = pct_delta_demographic_parity_logistic(X_reduced, Y1_labels, Y2_groups, C=C, cv=cv)\n",
    "\n",
    "    # Store results\n",
    "    return {\n",
    "        'Factor_Loading_Index': i,\n",
    "        'MMD': mmd_score,\n",
    "        '% Variance': var_pct,\n",
    "        '% Accuracy Mean': acc_mean,\n",
    "        '% Accuracy Std': acc_std,\n",
    "        'F1 Mean': f1_mean,\n",
    "        'F1 Std': f1_std,\n",
    "        '% Delta DP Mean': dp_mean,\n",
    "        '% Delta DP Std': dp_std\n",
    "    }\n",
    "\n",
    "# Modify run_simulation to explicitly pass all arguments\n",
    "def run_simulation(X, Y1_labels, Y2_groups, factor_loadings, cv=5, sample_frac=0.1, kernel='rbf', C=1.0, gamma='scale'):\n",
    "    \"\"\"\n",
    "    Runs the simulation for all factor loadings and stores results in a DataFrame.\n",
    "    \"\"\"\n",
    "    splits = get_splits(X, Y1_labels, cv=cv)  # Precompute cross-validation splits\n",
    "\n",
    "    # Use joblib to parallelize across factor loadings\n",
    "    results = Parallel(n_jobs=-1, verbose=10)(\n",
    "        delayed(process_factor_loading)(V, i, X, Y1_labels, Y2_groups, sample_frac, kernel, C, gamma, cv, splits) \n",
    "        for i, V in enumerate(factor_loadings)\n",
    "    )\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0de2c3d6-31e0-4bbe-a914-7cfcfdd0a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_loadings = list(Vs.values()) + list(Vs_fair.values())\n",
    "factor_loadings.append(V_PCA)\n",
    "factor_loadings.append(V_SIRY1)\n",
    "factor_loadings.append(V_negSIRY2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed39eec-bb37-47f1-a879-d0694f70d13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(factor_loadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5eb6aed6-4688-4c25-8e2b-0bf25e65f27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_frac = 0.1\n",
    "C = 1\n",
    "gamma = \"scale\"\n",
    "kernel = \"linear\"\n",
    "cv = 5\n",
    "splits = get_splits(X, Y, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b6d359-de23-48de-87fd-d792f52ecd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Wrap `enumerate(factor_loadings)` with `list()` so `tqdm` can track progress\n",
    "results = Parallel(n_jobs=-1, verbose=10)(\n",
    "    delayed(process_factor_loading)(V, i, X, Y, Z, sample_frac, kernel, C, gamma, cv, splits) \n",
    "    for i, V in tqdm(list(enumerate(factor_loadings)), total=len(factor_loadings))\n",
    ")\n",
    "\n",
    "# Convert list of results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save as a tab-separated text file\n",
    "results_df.to_csv(\"GERMAN_simulation_results.txt\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "703ea1e6-5511-4949-9b24-b4e2b1b3bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results from the saved file\n",
    "results_df = pd.read_csv(\"GERMAN_simulation_results.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc748313-e730-4c17-9f03-2534d1b8885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Track start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train a linear SVM on the full dataset using parallel cross-validation\n",
    "clf = SVC(kernel=\"linear\", C=1, random_state=42)\n",
    "accuracy_scores = cross_val_score(clf, X, Y, cv=5, n_jobs=-1)  # Enables parallel execution\n",
    "\n",
    "# Compute mean and standard deviation of accuracy\n",
    "accuracy_mean = accuracy_scores.mean()\n",
    "accuracy_std = accuracy_scores.std()\n",
    "\n",
    "# Track end time\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Prediction Accuracy on Full Dataset: {accuracy_mean:.4f} ± {accuracy_std:.4f}\")\n",
    "print(f\"Total Runtime: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c479e72c-a4f1-46cc-80e5-1172e5202feb",
   "metadata": {},
   "source": [
    "# Highlighting Key Results from results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c701959-1e77-40f0-a7bf-8d9d64c79f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the index ranges\n",
    "first_category = results_df.iloc[:169]\n",
    "second_category = results_df.iloc[169:219]  # Next 50 rows\n",
    "\n",
    "# Function to get required rows\n",
    "def get_selected_rows(df):\n",
    "    rows = []\n",
    "    # 1. The one with minimum MMD\n",
    "    rows.append(df.loc[df['MMD'].idxmin()])\n",
    "\n",
    "    # 2. Of the ones with MMD < 0.01, the one with maximum % Variance\n",
    "    subset = df[df['MMD'] < 0.01]\n",
    "    if not subset.empty:\n",
    "        rows.append(subset.loc[subset['% Variance'].idxmax()])\n",
    "\n",
    "    # 3. Of the ones with MMD < 0.06, the one with maximum % Accuracy Mean\n",
    "    subset = df[df['MMD'] < 0.06]\n",
    "    if not subset.empty:\n",
    "        rows.append(subset.loc[subset['% Accuracy Mean'].idxmax()])\n",
    "\n",
    "    # 4. The one with maximum % Accuracy Mean\n",
    "    rows.append(df.loc[df['% Accuracy Mean'].idxmax()])\n",
    "\n",
    "    # 5. Of the ones with MMD < 0.06, the one with maximum F1 Mean\n",
    "    subset = df[df['MMD'] < 0.06]\n",
    "    if not subset.empty:\n",
    "        rows.append(subset.loc[subset['F1 Mean'].idxmax()])\n",
    "\n",
    "    # 6. Of the ones with % Delta DP Mean < 0.01, the one with maximum % Variance\n",
    "    subset = df[df['% Delta DP Mean'] < 0.01]\n",
    "    if not subset.empty:\n",
    "        rows.append(subset.loc[subset['% Variance'].idxmax()])\n",
    "\n",
    "    # 7. Of the ones with % Delta DP Mean < 1, the one with maximum % Variance\n",
    "    subset = df[df['% Delta DP Mean'] < 1]\n",
    "    if not subset.empty:\n",
    "        rows.append(subset.loc[subset['% Variance'].idxmax()])\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Get selected rows for each category\n",
    "selected_first = get_selected_rows(first_category)\n",
    "selected_second = get_selected_rows(second_category)\n",
    "\n",
    "# Combine results\n",
    "selected_rows = pd.concat([selected_first, selected_second], ignore_index=True)\n",
    "\n",
    "round(selected_rows, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fa4ab5-1cf4-469f-892b-d13ef17c13ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters_list[c(16, 11, 104, 166, 23, 12)]\n",
    "\n",
    "selected_indices = [30, 11, 67, 117, 23, 12]\n",
    "selected_indices = [16, 11, 104, 166, 23, 12]\n",
    "\n",
    "# Extract elements\n",
    "selected_parameters = [parameters_list[i] for i in selected_indices]\n",
    "\n",
    "# Print or use the selected elements\n",
    "print(selected_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc5216a-b522-4c4d-8f75-3f158968a6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.iloc[selected_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acc5c99-9bf9-4a0d-a7ec-4fac6e5a95be",
   "metadata": {},
   "source": [
    "# Information Fairness Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4fcd363-fedf-4585-b2bd-c219a3d4b860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming results_df is already in memory\n",
    "# Define the fairness and information measures\n",
    "fairness_measures = [\"MMD\", \"% Delta DP Mean\"]\n",
    "information_measures = [\"% Variance\", \"% Accuracy Mean\", 'F1 Mean']\n",
    "\n",
    "# Define colors and labels|\n",
    "colors = [\"#377eb8\"] * 169 + [\"#e41a1c\"] * 50 + [\"#984ea3\", \"#ff7f00\", \"#4daf4a\"]\n",
    "labels = [\"SIRFairRe\"] * 169 + [\"FairPCA\"] * 50 + [\"PCA\", \"SIR(Y1)\", \"-SIR(Y2)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21514c7e-ff90-4df7-bb5f-4904fc54f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ab5f63-217c-4ade-b65c-4ac27361e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots\n",
    "accuracy_mean = 0.745\n",
    "\n",
    "for fairness in fairness_measures:\n",
    "    for info in information_measures:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        \n",
    "        # Plot each point with its assigned color and label\n",
    "        for i in range(len(results_df)):\n",
    "            plt.scatter(results_df[fairness][i], results_df[info][i],\n",
    "                        color=colors[i], label=labels[i] if i in [0, 169, 219, 220, 221] else \"_nolegend_\",\n",
    "                       s=150, alpha=0.8)\n",
    "\n",
    "        # Formatting\n",
    "        plt.xlabel(fairness, fontsize=20)\n",
    "        plt.ylabel(info, fontsize=20)\n",
    "        plt.xticks(fontsize=16)\n",
    "        plt.yticks(fontsize=16)\n",
    "        \n",
    "        # Add a horizontal line at accuracy_mean if the Y-axis represents accuracy\n",
    "        if \"Accuracy\" in info:\n",
    "            plt.axhline(y=accuracy_mean * 100, color=\"red\", linestyle=\"--\", linewidth=2, label=\"Raw Data Accuracy\")\n",
    "        \n",
    "        #plt.title(f\"{info} vs {fairness}\")\n",
    "        plt.legend(fontsize=14)\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Save the figure\n",
    "        filename = f\"GERMAN_SCATTER_{fairness.replace('% ', '').replace(' ', '_')}_vs_{info.replace('% ', '').replace(' ', '_')}.png\"\n",
    "        plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f9e2fd-1792-47f2-8975-b7561241e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_indices = [16, 166, 23, 12, 170, 193, 215]\n",
    "accuracy_mean = 0.745\n",
    "\n",
    "# Create scatter plots\n",
    "for fairness in fairness_measures:\n",
    "    for info in information_measures:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        \n",
    "        # Plot each point with its assigned color and label\n",
    "        for i in range(len(results_df)):\n",
    "            point_size = 600 if i in special_indices else 150  # Increase size for important points\n",
    "            plt.scatter(results_df[fairness][i], results_df[info][i],\n",
    "                        s=point_size,  # <- set the point size here\n",
    "                        color=colors[i], label=labels[i] if i in [0, 169, 219, 220, 221] else \"_nolegend_\",\n",
    "                       alpha=0.8)\n",
    "\n",
    "        # Formatting\n",
    "        plt.xlabel(fairness, fontsize=20)\n",
    "        plt.ylabel(info, fontsize=20)\n",
    "        plt.xticks(fontsize=16)\n",
    "        plt.yticks(fontsize=16)\n",
    "        \n",
    "        # Add a horizontal line at accuracy_mean if the Y-axis represents accuracy\n",
    "        if \"Accuracy\" in info:\n",
    "            plt.axhline(y=accuracy_mean * 100, color=\"red\", linestyle=\"--\", linewidth=2, label=\"Raw Data Accuracy\")\n",
    "        \n",
    "        #plt.title(f\"{info} vs {fairness}\")\n",
    "        plt.legend(fontsize=14)\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Save the figure\n",
    "        filename = f\"GERMAN_SCATTER_STARS_{fairness.replace('% ', '').replace(' ', '_')}_vs_{info.replace('% ', '').replace(' ', '_')}.png\"\n",
    "        plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ab6b03-20bc-4564-95b9-3f388c3cac5f",
   "metadata": {},
   "source": [
    "# Individual Metrics Visualized with Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c04164c-a014-4848-bb48-de1546a22182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the file\n",
    "file_path = \"GERMAN_simulation_results.txt\"  # Update this if needed\n",
    "df = pd.read_csv(file_path, sep=\"\\t\")  # Read the tab-separated values\n",
    "\n",
    "# Define the parameter grid\n",
    "grid_size = (13, 13)  # 8x8 grid\n",
    "alpha_values = sorted(set(param[0] for param in parameters_list))  # Unique alpha values\n",
    "lambda_values = sorted(set(param[1] for param in parameters_list))  # Unique lambda values\n",
    "\n",
    "# Map parameter settings to grid indices\n",
    "param_to_index = {tuple(parameters_list[i]): i for i in range(len(parameters_list))}\n",
    "\n",
    "# Define characteristics to plot\n",
    "characteristics = {\n",
    "    \"MMD\": \"MMD\",\n",
    "    \"% Variance\": \"% Variance\",\n",
    "    \"% Accuracy Mean\": \"% Accuracy Mean\",\n",
    "    \"% Delta DP Mean\": \"% Delta DP Mean\",\n",
    "    \"F1 Mean\": \"F1 Mean\"\n",
    "}\n",
    "\n",
    "# Choose a single-color colormap (e.g., \"Blues\", \"Greens\", \"Reds\", \"Purples\")\n",
    "cmap_color = \"Blues\"  \n",
    "\n",
    "# Create an 8x8 grid for each heatmap\n",
    "for char_name, col_name in characteristics.items():\n",
    "    heatmap_data = np.full(grid_size, np.nan)  # Initialize grid with NaNs for missing values\n",
    "    \n",
    "    # Populate the heatmap matrix based on parameter mappings\n",
    "    for (alpha, lambda_), index in param_to_index.items():\n",
    "        if index in df.index:\n",
    "            i = alpha_values.index(alpha)\n",
    "            j = lambda_values.index(lambda_)\n",
    "            heatmap_data[i, j] = df.at[index, col_name]  # Assign corresponding value\n",
    "    \n",
    "    # Set rounding format\n",
    "    fmt = \".3f\" if char_name == \"MMD\" else \".1f\"\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    ax = sns.heatmap(heatmap_data, annot=True, fmt=fmt, cmap=cmap_color,\n",
    "                     xticklabels=lambda_values, yticklabels=alpha_values)\n",
    "\n",
    "    # Formatting\n",
    "    plt.xlabel(\"Lambda\", fontsize=20)\n",
    "    plt.ylabel(\"Alpha\", fontsize=20)\n",
    "    plt.title(f\"Heatmap of {char_name}\", fontsize=20)\n",
    "    plt.xticks(rotation=45, fontsize=16)\n",
    "    plt.yticks(rotation=0, fontsize=16)\n",
    "\n",
    "    # Save the heatmap as an image file\n",
    "    filename = f\"GERMAN_HEATMAP_{char_name.replace('% ', '').replace(' ', '_')}.png\"\n",
    "    plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88695547-dff0-4584-9188-553e787e6fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tab-separated text file into a DataFrame\n",
    "results_df = pd.read_csv(\"GERMAN_simulation_results.txt\", sep=\"\\t\")\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69de6768-205b-4974-b8d7-76dad72f433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scores = []\n",
    "count = 0\n",
    "for factor_loading in factor_loadings:\n",
    "    print(count)\n",
    "    X_reduced = X @ factor_loading\n",
    "    score = silhouette_score(X_reduced, Y)\n",
    "    silhouette_scores.append(score)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe0d969-c493-4609-a10b-366353c0eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Silhouette Scores as a new column in results_df\n",
    "results_df[\"Silhouette Score\"] = silhouette_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83fa814-642f-472a-ae5e-b0b5a394a80b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4664ad53-3e03-4679-b006-125b28121a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.iloc[80]  # Access by integer index position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11620e1c-f8c2-4166-a452-6151dda34e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sirfairre = results_df.iloc[:169].loc[results_df.iloc[:169][results_df.iloc[:169][\"MMD\"] <= 0.25][\"% Accuracy Mean\"].idxmax()]\n",
    "best_fairpca = results_df.iloc[169:219].loc[results_df.iloc[169:219][results_df.iloc[169:219][\"MMD\"] <= 0.25][\"% Accuracy Mean\"].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5882c0-69c3-4260-a38f-9fc8b934b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sirfairre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bde56e0-0b6f-4166-9940-94a1e27838c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fairpca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07aa7369-4f39-477f-91b5-01d3419524da",
   "metadata": {},
   "source": [
    "# Reduced Dim Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b1645e-0bab-4fca-978a-62301f45cb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reduced_X(X, factor_loadings[12], Z, \"SIRFairRe\", \"Age\", \">= 25\", \"< 25\", caption=\"MMD = 0.019; ΔDP = 0.1%\")\n",
    "plot_reduced_X(X, factor_loadings[12], Y, \"SIRFairRe\", \"Credit Risk\", \"Good Credit Risk\", \"Bad Credit Risk\", caption=\"Var = 11.2%; Acc = 69.9%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142b667b-0828-4a01-81fe-eb7a0e92c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reduced_X(X, factor_loadings[170], Z, \"FairPCA\", \"Age\", \">= 25\", \"< 25\", caption=\"MMD = 0.085; ΔDP = 5.3%\")\n",
    "plot_reduced_X(X, factor_loadings[170], Y, \"FairPCA\", \"Credit Risk\", \"Good Credit Risk\", \"Bad Credit Risk\", caption=\"Var = 11.3%; Acc = 72.4%; SS = 0.055\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006efe18-0f00-4bdb-9712-00aee1f107fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define indices for PCA, SIR(Y1), and -SIR(Y2)\n",
    "factor_indices = {\"PCA\": 219, \"SIR(Y1)\": 220, \"-SIR(Y2)\": 221}\n",
    "\n",
    "# Define target variable settings with standardized caption names\n",
    "targets = [\n",
    "    (\"Age\", \">= 25\", \"< 25\", \"MMD\", \"% Delta DP Mean\"),  # MMD and ΔDP for Gender\n",
    "    (\"Credit Risk\", \"Good Credit Risk\", \"Bad Credit Risk\", \"% Variance\", \"% Accuracy Mean\", \"% Delta DP Mean\")  # Variance, Accuracy, and ΔDP for Income\n",
    "]\n",
    "\n",
    "# Define standardized caption name replacements\n",
    "caption_renames = {\n",
    "    \"MMD\": \"MMD\",\n",
    "    \"% Variance\": \"Var\",\n",
    "    \"% Accuracy Mean\": \"Acc\",\n",
    "    \"% Delta DP Mean\": \"ΔDP\"\n",
    "}\n",
    "\n",
    "# Generate plots\n",
    "for method_name, idx in factor_indices.items():\n",
    "    for target, group1, group2, *metric_keys in targets:\n",
    "        \n",
    "        # Extract corresponding performance metrics\n",
    "        metrics = results_df.loc[results_df[\"Factor_Loading_Index\"] == idx, metric_keys].values.flatten()\n",
    "\n",
    "        # Format caption dynamically with different rounding for each metric\n",
    "        formatted_metrics = []\n",
    "        for i, key in enumerate(metric_keys):\n",
    "            new_name = caption_renames[key]\n",
    "\n",
    "            if new_name == \"MMD\":  # Round to 3 decimal places\n",
    "                formatted_metrics.append(f\"{new_name} = {metrics[i]:.3f}\")\n",
    "            elif new_name == \"Var\":  # Round to 2 decimal places and add '%'\n",
    "                formatted_metrics.append(f\"{new_name} = {metrics[i]:.2f}%\")\n",
    "            elif new_name in [\"Acc\", \"ΔDP\"]:  # Round to 1 decimal place and add '%'\n",
    "                formatted_metrics.append(f\"{new_name} = {metrics[i]:.1f}%\")\n",
    "            else:  # Default case (shouldn't happen)\n",
    "                formatted_metrics.append(f\"{new_name} = {metrics[i]}\")\n",
    "\n",
    "        caption = \"; \".join(formatted_metrics)\n",
    "\n",
    "        # Generate plot\n",
    "        plot_reduced_X(X, factor_loadings[idx], Z if target == \"Gender\" else Y, method_name, target, group1, group2, caption=caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff9753b-cdf3-46ad-9dad-bf5a3d91da05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb219f2-3fd3-4743-b0bf-de4a1160dd3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a595ea-b4ed-4fcc-9ff4-45f14f03133e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1578f807-55a9-460c-84ac-2c2992157015",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
