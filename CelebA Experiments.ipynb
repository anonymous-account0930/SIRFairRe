{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9802dc91-79ab-480a-9037-7737f7cc1946",
   "metadata": {},
   "source": [
    "# Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecd41c87-c2e1-41eb-b0c1-312beb7d3d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import eigh, block_diag, pinv\n",
    "from scipy.linalg import eig\n",
    "import torch\n",
    "import pymanopt\n",
    "from pymanopt.manifolds import Product, Stiefel\n",
    "from pymanopt import Problem\n",
    "from pymanopt.optimizers import ConjugateGradient\n",
    "import autograd\n",
    "from scipy.linalg import qr\n",
    "from numpy.linalg import qr\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.linalg import eigh, block_diag, pinv\n",
    "import torch\n",
    "import pymanopt\n",
    "from pymanopt.manifolds import Product, Stiefel\n",
    "from pymanopt import Problem\n",
    "from pymanopt.optimizers import ConjugateGradient\n",
    "import autograd\n",
    "from scipy.stats import ortho_group\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6018f191-453b-44ee-9aa7-a1c08e9b4578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac67ff4f-aad4-492e-b2b6-2a12c860ddf3",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36522c3c-5836-4d87-a299-538eefd512c4",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee06feaa-6094-4348-9c52-4f022401ff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def method1(X, Y1, Y2, d1, a, l):\n",
    "    \"\"\"\n",
    "    Compute the desired matrices and eigenvectors as described in the prompt.\n",
    "\n",
    "    Parameters:\n",
    "        X (np.ndarray): Data matrix (n x p)\n",
    "        Y1 (np.ndarray): Binary response vector of length n\n",
    "        Y2 (np.ndarray): Binary response vector of length n\n",
    "        d1 (int): Number of eigenvalues/vectors for the first block\n",
    "        d2 (int): Number of eigenvalues/vectors for the second block\n",
    "        a (float): hyperparameter for contrastive term\n",
    "        l (float): hyperparameter for PCA term\n",
    "    Returns:\n",
    "        tuple: V1, V2 matrices corresponding to the extracted eigenvectors\n",
    "    \"\"\"\n",
    "    # Total number of eigenvalues/vectors to extract\n",
    "    d = d1\n",
    "    # Check dimensions\n",
    "    n, p = X.shape\n",
    "    assert len(Y1) == n and len(Y2) == n, \"Response vectors must have length n\"\n",
    "    assert d < p, \"d must be less than the number of features p\"\n",
    "    assert a >= 0, \"contrastive hyperparameter must be nonnegative\"\n",
    "    assert l >= 0, \"PCA hyperparameter must be nonnegative\"\n",
    "    \n",
    "    # Compute sample covariance of X\n",
    "    Sigma = np.cov(X, rowvar=False)\n",
    "    \n",
    "    epsilon = 1e-4  # Small positive value\n",
    "    Sigma_inv = np.linalg.inv(Sigma + epsilon * np.eye(Sigma.shape[0]))\n",
    "    \n",
    "    print(1)\n",
    "\n",
    "    def compute_sigma_1(X, Y):\n",
    "        \"\"\"Compute Sigma_1 (or Sigma_2) as described.\"\"\"\n",
    "        p_1 = np.mean(Y)  # Proportion of slice Y = 1\n",
    "        m_1 = np.mean(X[Y == 1], axis=0)  # Mean for Y = 1\n",
    "        m_0 = np.mean(X[Y == 0], axis=0)  # Mean for Y = 0\n",
    "        \n",
    "        # Compute Sigma_1\n",
    "        Sigma_1 = p_1 * np.outer(m_1, m_1) + (1 - p_1) * np.outer(m_0, m_0)\n",
    "        return Sigma_1\n",
    "\n",
    "    # Compute Sigma_1, Sigma_2, and Sigma_12\n",
    "    Sigma_1 = compute_sigma_1(X, Y1)\n",
    "    Sigma_2 = compute_sigma_1(X, Y2)\n",
    "    \n",
    "    print(2)\n",
    "    \n",
    "    # Compute A_1, A_2, A_12\n",
    "    A_1 = Sigma @ Sigma_1 @ Sigma\n",
    "    A_2 = Sigma @ Sigma_2 @ Sigma\n",
    "    \n",
    "    print(3)\n",
    "    \n",
    "    # Compute matrix to find top eigenvectors of\n",
    "    M1 = Sigma_inv @ (a * A_1 - A_2) @ Sigma_inv + l * Sigma_inv\n",
    "    M2 = Sigma_inv @ Sigma_inv\n",
    "    \n",
    "    M1 = (M1 + M1.T)/2\n",
    "    M2 = (M2 + M2.T)/2\n",
    "    \n",
    "    print(4)\n",
    "    \n",
    "    #print(\"Symmetry check for M1:\", np.allclose(M1, M1.T))\n",
    "    #print(\"Symmetry check for M2:\", np.allclose(M2, M2.T))\n",
    "\n",
    "    \n",
    "    #M2_cond_number = np.linalg.cond(M2)\n",
    "    #print(\"M2 condition number = \", end=\"\")\n",
    "    #print(M2_cond_number)\n",
    "    \n",
    "    # Compute eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = eigh(M1, M2)\n",
    "    \n",
    "    print(5)\n",
    "    \n",
    "    # Coerce eigenvalues and eigenvectors to real values\n",
    "    eigenvalues = np.real(eigenvalues)\n",
    "    eigenvectors = np.real(eigenvectors)\n",
    "    \n",
    "    # Sort eigenvalues and eigenvectors in descending order\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    eigenvectors = eigenvectors[:, sorted_indices]\n",
    "    \n",
    "    #print(\"all eigenvalues = \", end=\"\")\n",
    "    #print(eigenvalues)\n",
    "    \n",
    "    # Select the top eigenvectors (e.g., top d1 eigenvectors)\n",
    "    top_eigenvalues = eigenvalues[:d1]\n",
    "    top_eigenvectors = eigenvectors[:, :d1]\n",
    "    \n",
    "    print(6)\n",
    "    \n",
    "    #print(\"top eigenvalues = \", end=\"\")\n",
    "    #print(top_eigenvalues)\n",
    "    \n",
    "    #print(\"pct var = \", end=\"\")\n",
    "    #print(sum(top_eigenvalues) / sum(eigenvalues) * 100)\n",
    "    \n",
    "    # Rotate back\n",
    "    V = Sigma_inv @ top_eigenvectors\n",
    "    \n",
    "    print(7)\n",
    "    \n",
    "    # Perform QR decomposition to orthogonalize V\n",
    "    Q, R = np.linalg.qr(V)\n",
    "    \n",
    "    return np.real(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe195481-5e80-4d1a-ac0d-13f24f70c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def method2(X, Y1, Y2, d1, d2, e, a, b, c, l):\n",
    "    \"\"\"\n",
    "    Compute the desired matrices and eigenvectors as described in the prompt.\n",
    "\n",
    "    Parameters:\n",
    "        X (np.ndarray): Data matrix (n x p)\n",
    "        Y1 (np.ndarray): Binary response vector of length n\n",
    "        Y2 (np.ndarray): Binary response vector of length n\n",
    "        d1 (int): Number of eigenvalues/vectors for the first block\n",
    "        d2 (int): Number of eigenvalues/vectors for the second block\n",
    "        a (float): hyperparameter for contrastive term (alpha)\n",
    "        b (float): hyperparameter for V2 (beta)\n",
    "        c (float): hyperparameter for V2 contrastive term (gamma)\n",
    "        l (float): hyperparameter for PCA term (lambda)\n",
    "    Returns:\n",
    "        tuple: V1, V2 matrices corresponding to the extracted eigenvectors\n",
    "    \"\"\"\n",
    "    # Total number of eigenvalues/vectors to extract\n",
    "    d = d1 + d2\n",
    "    # Check dimensions\n",
    "    n, p = X.shape\n",
    "    assert len(Y1) == n and len(Y2) == n, \"Response vectors must have length n\"\n",
    "    assert d < p, \"d must be less than the number of features p\"\n",
    "    assert a >= 0, \"contrastive hyperparameter must be nonnegative\"\n",
    "    assert b >= 0, \"hyperparameter for V2 must be nonnegative\"\n",
    "    assert c >= 0, \"hyperparameter for V2 contrastive term must be nonnegative\"\n",
    "    assert l >= 0, \"PCA hyperparameter must be nonnegative\"\n",
    "    assert e >= 0, \"first hyperparameter must be nonnegative\"\n",
    "    assert max(a, b, c, l, e) >= 0, \"at least one hyperparameter must be positive\"\n",
    "    \n",
    "    # Compute sample covariance of X\n",
    "    Sigma = np.cov(X, rowvar=False)\n",
    "    #Sigma_inv = np.linalg.inv(Sigma)\n",
    "    B = Sigma @ Sigma\n",
    "\n",
    "    def compute_sigma_1(X, Y):\n",
    "        \"\"\"Compute Sigma_1 (or Sigma_2) as described.\"\"\"\n",
    "        p_1 = np.mean(Y)  # Proportion of slice Y = 1\n",
    "        m_1 = np.mean(X[Y == 1], axis=0)  # Mean for Y = 1\n",
    "        m_0 = np.mean(X[Y == 0], axis=0)  # Mean for Y = 0\n",
    "        \n",
    "        # Compute Sigma_1\n",
    "        Sigma_1 = p_1 * np.outer(m_1, m_1) + (1 - p_1) * np.outer(m_0, m_0)\n",
    "        return Sigma_1\n",
    "\n",
    "    # Compute Sigma_1, Sigma_2, and Sigma_12\n",
    "    Sigma_1 = compute_sigma_1(X, Y1)\n",
    "    Sigma_2 = compute_sigma_1(X, Y2)\n",
    "    \n",
    "    # Compute A_1, A_2, A_12\n",
    "    A_1 = Sigma @ Sigma_1 @ Sigma\n",
    "    A_2 = Sigma @ Sigma_2 @ Sigma\n",
    "    \n",
    "    # V1\n",
    "    # Compute matrix to find top eigenvectors of\n",
    "    M1 = Sigma @ (A_1 - a * A_2) @ Sigma\n",
    "    \n",
    "    # Compute eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors1 = np.linalg.eig(M1)\n",
    "    \n",
    "    # Sort eigenvalues and eigenvectors in descending order\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    eigenvectors1 = eigenvectors1[:, sorted_indices]\n",
    "    \n",
    "    # Select the top eigenvectors (e.g., top d1 eigenvectors)\n",
    "    top_eigenvectors1 = eigenvectors1[:, :d1]\n",
    "    \n",
    "    # Rotate back\n",
    "    V1 = np.linalg.solve(Sigma, top_eigenvectors1)\n",
    "    \n",
    "    # Perform QR decomposition to orthogonalize V\n",
    "    Q1, R = np.linalg.qr(V1)\n",
    "    \n",
    "    \n",
    "    # V2\n",
    "    # Compute matrix to find top eigenvectors of\n",
    "    M2 = Sigma @ (c * A_2 - b * A_1) @ Sigma\n",
    "    \n",
    "    # Compute eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors2 = np.linalg.eig(M2)\n",
    "    \n",
    "    # Sort eigenvalues and eigenvectors in descending order\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    eigenvectors2 = eigenvectors2[:, sorted_indices]\n",
    "    \n",
    "    # Select the top eigenvectors (e.g., top d1 eigenvectors)\n",
    "    top_eigenvectors2 = eigenvectors2[:, :d1]\n",
    "    \n",
    "    # Rotate back\n",
    "    V2 = np.linalg.solve(Sigma, top_eigenvectors2)\n",
    "    \n",
    "    # Perform QR decomposition to orthogonalize V\n",
    "    Q2, R = np.linalg.qr(V2)\n",
    "    \n",
    "    if l == 0:\n",
    "        return np.real(Q1), np.real(Q2)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Convert necessary matrices to PyTorch tensors\n",
    "        Sigma_cubed = torch.tensor(Sigma @ B, dtype=torch.float64)\n",
    "        A_1_torch = torch.tensor(A_1, dtype=torch.float64)\n",
    "        A_2_torch = torch.tensor(A_2, dtype=torch.float64)\n",
    "        B_torch = torch.tensor(B, dtype=torch.float64)\n",
    "        \n",
    "        # Define the product manifold\n",
    "        manifold = pymanopt.manifolds.Product([\n",
    "            pymanopt.manifolds.Stiefel(p, d1),\n",
    "            pymanopt.manifolds.Stiefel(p, d2)\n",
    "        ])\n",
    "\n",
    "        \n",
    "        @pymanopt.function.pytorch(manifold)\n",
    "        def cost(V_1, V_2):\n",
    "            V_combined = torch.cat([V_1, V_2], dim=1)\n",
    "            part1 = -torch.trace(V_1.T @ A_1_torch @ V_1)\n",
    "            part2 = -torch.trace(V_2.T @ A_2_torch @ V_2)\n",
    "            VDV = V_combined.T @ B_torch @ V_combined\n",
    "            VDV_inv = torch.linalg.pinv(VDV)\n",
    "            part3 = -l * torch.trace(V_combined.T @ Sigma_cubed @ V_combined @ VDV_inv)\n",
    "            return part1 + part2 + part3\n",
    "\n",
    "        # Create the optimization problem\n",
    "        problem = Problem(manifold=manifold, cost=cost)\n",
    "\n",
    "        # Use a conjugate gradient optimizer\n",
    "        optimizer = ConjugateGradient()\n",
    "        result = optimizer.run(problem)\n",
    "\n",
    "        # Extract optimized V_1 and V_2\n",
    "        V_1_opt, V_2_opt = result.point\n",
    "        \n",
    "        # Rotate back\n",
    "        V1 = np.linalg.solve(Sigma, V_1_opt)\n",
    "        V2 = np.linalg.solve(Sigma, V_2_opt)\n",
    "        \n",
    "        # Perform QR decomposition to orthogonalize V\n",
    "        Q1, R = np.linalg.qr(V1)\n",
    "        Q2, R = np.linalg.qr(V2)\n",
    "        \n",
    "        return Q1, Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d00ce291-9396-483d-a8f0-d863835db619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairPCA(X, Y, d, m):\n",
    "    # Compute the covariance matrix\n",
    "    Sigma = np.cov(X, rowvar=False)\n",
    "    \n",
    "    # Split the data into two groups based on Y\n",
    "    X0 = X[Y == 0]\n",
    "    X1 = X[Y == 1]\n",
    "    \n",
    "    # Compute mean vectors\n",
    "    mu0 = np.mean(X0, axis=0)\n",
    "    mu1 = np.mean(X1, axis=0)\n",
    "    \n",
    "    # Compute covariance matrices\n",
    "    Sigma0 = np.cov(X0, rowvar=False)\n",
    "    Sigma1 = np.cov(X1, rowvar=False)\n",
    "    \n",
    "    # Compute the difference of covariance matrices\n",
    "    Sigma_diff = Sigma0 - Sigma1\n",
    "    \n",
    "    # Compute eigenvalues and eigenvectors of Sigma_diff\n",
    "    eigvals, eigvecs = np.linalg.eigh(Sigma_diff)\n",
    "    \n",
    "    # Sort eigenvectors by absolute eigenvalues in descending order\n",
    "    sorted_indices = np.argsort(-np.abs(eigvals))\n",
    "    top_m_eigvecs = eigvecs[:, sorted_indices[:m]]\n",
    "    \n",
    "    # Include mu0 - mu1 in the subspace\n",
    "    mu_diff = (mu0 - mu1).reshape(-1, 1)\n",
    "    combined_basis = np.hstack([top_m_eigvecs, mu_diff])\n",
    "    \n",
    "    # Perform QR decomposition to obtain an orthonormal basis\n",
    "    Q, _ = np.linalg.qr(combined_basis, mode='reduced')\n",
    "    Pi = Q @ Q.T  # Projection matrix\n",
    "    \n",
    "    # Compute Pi Sigma Pi\n",
    "    transformed_Sigma = Pi @ Sigma @ Pi\n",
    "    \n",
    "    # Compute the top d eigenvectors of Pi Sigma Pi\n",
    "    eigvals_final, eigvecs_final = np.linalg.eigh(transformed_Sigma)\n",
    "    sorted_indices_final = np.argsort(-eigvals_final)\n",
    "    top_d_eigvecs = eigvecs_final[:, sorted_indices_final[:d]]\n",
    "    \n",
    "    return top_d_eigvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d426fc3-7b50-452a-907f-710343beaf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIR(X, Y, d):\n",
    "    # Compute sample covariance of X\n",
    "    Sigma = np.cov(X, rowvar=False)\n",
    "    \n",
    "    epsilon = 1e-5  # Small positive value\n",
    "    Sigma_inv = np.linalg.inv(Sigma + epsilon * np.eye(Sigma.shape[0]))\n",
    "\n",
    "    def compute_sigma_1(X, Y):\n",
    "        \"\"\"Compute Sigma_1 (or Sigma_2) as described.\"\"\"\n",
    "        p_1 = np.mean(Y)  # Proportion of slice Y = 1\n",
    "        m_1 = np.mean(X[Y == 1], axis=0)  # Mean for Y = 1\n",
    "        m_0 = np.mean(X[Y == 0], axis=0)  # Mean for Y = 0\n",
    "        \n",
    "        # Compute Sigma_1\n",
    "        Sigma_1 = p_1 * np.outer(m_1, m_1) + (1 - p_1) * np.outer(m_0, m_0)\n",
    "        return Sigma_1\n",
    "\n",
    "    # Compute Sigma_1, Sigma_2, and Sigma_12\n",
    "    Sigma_1 = compute_sigma_1(X, Y)\n",
    "    \n",
    "    # Compute A_1, A_2, A_12\n",
    "    A_1 = Sigma @ Sigma_1 @ Sigma\n",
    "    \n",
    "    # Compute matrix to find top eigenvectors of\n",
    "    M = Sigma_inv @ A_1 @ Sigma_inv\n",
    "    \n",
    "    M = (M + M.T)/2\n",
    "\n",
    "    # Compute eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = eig(M)\n",
    "    \n",
    "    # Coerce eigenvalues and eigenvectors to real values\n",
    "    eigenvalues = np.real(eigenvalues)\n",
    "    eigenvectors = np.real(eigenvectors)\n",
    "    \n",
    "    # Sort eigenvalues and eigenvectors in descending order\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    eigenvectors = eigenvectors[:, sorted_indices]\n",
    "    \n",
    "    # Select the top eigenvectors (e.g., top d1 eigenvectors)\n",
    "    top_eigenvalues = eigenvalues[:d]\n",
    "    top_eigenvectors = eigenvectors[:, :d]\n",
    "    \n",
    "    # Rotate back\n",
    "    V = Sigma_inv @ top_eigenvectors\n",
    "    \n",
    "    # Perform QR decomposition to orthogonalize V\n",
    "    Q, R = np.linalg.qr(V)\n",
    "    \n",
    "    return np.real(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39a0052-95f7-4a6c-86c9-729b0c42c272",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87c7e133-43dd-4a10-ae78-59803c98139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmd(X1, X2, sigma, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Compute the Maximum Mean Discrepancy (MMD) between two datasets X1 and X2 with memory efficiency.\n",
    "    \"\"\"\n",
    "    def rbf_batch(X1, X2, sigma):\n",
    "        \"\"\"\n",
    "        Compute the Radial Basis Function (RBF) kernel between two datasets in batches.\n",
    "        \"\"\"\n",
    "        result = 0.0\n",
    "        for i in range(0, X1.shape[0], batch_size):\n",
    "            X1_batch = X1[i:i + batch_size]\n",
    "            distances = cdist(X1_batch, X2, 'sqeuclidean')\n",
    "            result += np.sum(np.exp(-distances / (2 * sigma**2)))\n",
    "        return result\n",
    "\n",
    "    m = X1.shape[0]\n",
    "    n = X2.shape[0]\n",
    "\n",
    "    # Term 1: Intra-set distances for X1\n",
    "    term1 = (1 / m**2) * rbf_batch(X1, X1, sigma)\n",
    "\n",
    "    # Term 2: Intra-set distances for X2\n",
    "    term2 = (1 / n**2) * rbf_batch(X2, X2, sigma)\n",
    "\n",
    "    # Term 3: Inter-set distances between X1 and X2\n",
    "    term3 = (2 / (m * n)) * rbf_batch(X1, X2, sigma)\n",
    "\n",
    "    d = term1 + term2 - term3\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ef2de2b-f5bf-43c0-b858-2a758135dfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_var(X, V):\n",
    "    \"\"\"\n",
    "    Compute the % of variance retained by the projection in the reduced dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute sample covariance of X\n",
    "    Sigma = np.cov(X, rowvar=False)\n",
    "    \n",
    "    return 100 * np.trace(V.T @ Sigma @ V) / np.trace(Sigma) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1362df6-3ee7-4c5d-aadd-fbef2c4d70df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_accuracy(X_reduced_data, Y1_labels, kernel='rbf', C=1.0, gamma='scale', cv=5):\n",
    "    \"\"\"\n",
    "    Compute the cross-validated accuracy (%Acc) for a kernel SVM.\n",
    "    \n",
    "    Parameters:\n",
    "    - reduced_data: np.ndarray, the reduced data matrix (n_samples, n_features)\n",
    "    - labels: np.ndarray, the target labels (n_samples,)\n",
    "    - kernel: str, the kernel type to use for SVM ('linear', 'rbf', etc.)\n",
    "    - C: float, regularization parameter for SVM\n",
    "    - gamma: str or float, kernel coefficient for 'rbf', 'poly', and 'sigmoid'\n",
    "    - cv: int, the number of cross-validation folds\n",
    "\n",
    "    Returns:\n",
    "    - mean_accuracy: float, mean accuracy across folds\n",
    "    - std_accuracy: float, standard deviation of accuracy across folds\n",
    "    \"\"\"\n",
    "    svm = SVC(kernel=kernel, C=C, gamma=gamma)\n",
    "    accuracy_scores = cross_val_score(svm, X_reduced_data, Y1_labels, cv=cv, scoring='accuracy')\n",
    "    mean_accuracy = np.mean(accuracy_scores)\n",
    "    std_accuracy = np.std(accuracy_scores)\n",
    "    return mean_accuracy * 100, std_accuracy * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe71e1c3-9da3-4250-8a68-9ab2a0cf9107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_delta_demographic_parity(X_reduced_data, Y1_labels, Y2_groups, kernel='rbf', C=1.0, gamma='scale', cv=5):\n",
    "    \"\"\"\n",
    "    Compute %∆DP (Demographic Parity) for a kernel SVM.\n",
    "\n",
    "    Parameters:\n",
    "    - X_reduced_data: np.ndarray, the reduced data matrix (n_samples, n_features)\n",
    "    - Y2_labels: np.ndarray, the target labels (n_samples,)\n",
    "    - groups: np.ndarray, the demographic group labels (n_samples,)\n",
    "    - kernel: str, the kernel type to use for SVM ('linear', 'rbf', etc.)\n",
    "    - C: float, regularization parameter for SVM\n",
    "    - gamma: str or float, kernel coefficient for 'rbf', 'poly', and 'sigmoid'\n",
    "    - cv: int, the number of cross-validation folds\n",
    "\n",
    "    Returns:\n",
    "    - mean_dp: float, mean demographic parity (%∆DP) across folds\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    svm = SVC(kernel=kernel, C=C, gamma=gamma)\n",
    "\n",
    "    dp_differences = []\n",
    "\n",
    "    for train_index, test_index in skf.split(X_reduced_data, Y1_labels):\n",
    "        X_train, X_test = X_reduced_data[train_index], X_reduced_data[test_index]\n",
    "        y_train, y_test = Y1_labels[train_index], Y1_labels[test_index]\n",
    "        g_test = Y2_groups[test_index]\n",
    "\n",
    "        svm.fit(X_train, y_train)\n",
    "        y_pred = svm.predict(X_test)\n",
    "\n",
    "        # Compute positive prediction rates per group\n",
    "        positive_rates = {}\n",
    "        for group in np.unique(g_test):\n",
    "            group_indices = np.where(g_test == group)[0]\n",
    "            positive_rate = np.mean(y_pred[group_indices] == 1)\n",
    "            positive_rates[group] = positive_rate\n",
    "\n",
    "        # Compute absolute differences between groups\n",
    "        group_rates = list(positive_rates.values())\n",
    "        dp_difference = abs(group_rates[0] - group_rates[1])  # Adjust for two-group assumption\n",
    "        dp_differences.append(dp_difference)\n",
    "\n",
    "    mean_dp = np.mean(dp_differences) * 100  # Convert to percentage\n",
    "    sd_dp = np.std(dp_differences) * 100 # Convert to percentage\n",
    "    return mean_dp, sd_dp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efbfed3-9679-4f99-ac80-56cc4270c511",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load CelebA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e93ff30f-e5cc-4870-b6d0-724f2336188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_subset = np.load(\"Y_subset.npy\")\n",
    "Z_subset = np.load(\"Z_subset.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "500e9d42-10d7-440c-9d9c-b81d7599e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_subset = np.load(\"X_subset.npy\")\n",
    "#X_subset2 = np.load(\"X_subset2.npy\")\n",
    "#X_subset3 = np.load(\"X_subset3.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a70f6146-1782-4235-8c84-bcd553793366",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normalized = (X_subset - 127.5) / 127.5  # Scale to [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "37ece6cb-1cec-44ad-802a-da75750a44d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize a StandardScaler for each subset\n",
    "#scaler1 = StandardScaler()\n",
    "#scaler2 = StandardScaler()\n",
    "#scaler3 = StandardScaler()\n",
    "\n",
    "# Fit and transform each subset separately\n",
    "#X_subset1_scaled = scaler1.fit_transform(X_subset1)\n",
    "#X_subset2_scaled = scaler2.fit_transform(X_subset2)\n",
    "#X_subset3_scaled = scaler3.fit_transform(X_subset3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043ebb89-d6b3-4617-a088-df9aa9973907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "V = method1(X_normalized, Y_subset, Z_subset, 500, 0.01, 10)\n",
    "\n",
    "#V1 = method1(X_subset1_scaled, Y_subset, Z_subset, 20, 0.5, 10)\n",
    "#V2 = method1(X_subset2_scaled, Y_subset, Z_subset, 20, 0.5, 10)\n",
    "#V3 = method1(X_subset3_scaled, Y_subset, Z_subset, 20, 0.5, 10)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"method1 execution time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Time the saving process\n",
    "save_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99296c1a-7a5c-4463-a778-a7a8786c2bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"V_prenormalized_celebAd=500_.01_10.npy\", V)\n",
    "#np.save(\"V2_celebAd=20_0.5_10.npy\", V2)\n",
    "#np.save(\"V3_celebAd=20_0.5_10.npy\", V3)\n",
    "save_elapsed_time = time.time() - save_start_time\n",
    "print(f\"Saving time: {save_elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51bb1266-2e6c-4003-a600-5f093d4e6053",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V1V1T = V1 @ V1.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2750e695-08b2-4025-be19-84606e98f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "VVT = V @ V.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e688cc0-7afc-4881-b9e1-1a2ce3e3f950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V2V2T = V2 @ V2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "359e3055-e102-46ea-abf7-f39670903697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V3V3T = V3 @ V3.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "719d957e-2d47-4499-9eaa-5508e95e635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean = X_subset.mean(axis=0)  # Compute the mean of the original data\n",
    "X_centered = X_normalized - X_mean  # Center the data\n",
    "\n",
    "X_reduced = (X_centered @ VVT) + X_mean  # Apply projection and add back the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4670253-c2d4-4237-b04b-035ce9be9e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_subset1_reduced = X_subset1 @ V1V1T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3da6be80-4685-4813-b3c1-835e1d54cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_subset2_reduced = X_subset2 @ V2V2T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4867a405-62da-4206-9321-d0b64ae76077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_subset3_reduced = X_subset3 @ V3V3T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ab712c-85ab-4c04-bb0b-48026e39a8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(X_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a65be3-0802-4df7-8191-c04cd0d03138",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(X_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae70b90-22d7-4205-b376-8d3262568c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d232bf9d-ae15-4fb8-9871-6e32b9325830",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "513fea60-7833-4bbf-8554-940d833fa971",
   "metadata": {},
   "outputs": [],
   "source": [
    "low, high = np.percentile(X_reduced, [1, 99])  # Get robust min/max\n",
    "X_reduced = np.clip(X_reduced, low, high)  # Clip extreme outliers\n",
    "X_reduced = (X_reduced - low) / (high - low) * 255  # Rescale to 0-255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e0ad0074-d22a-456e-94a9-31ea72f272cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = X_reduced.mean()\n",
    "std = X_reduced.std()\n",
    "X_reduced = np.clip(X_reduced, mean - 2 * std, mean + 2 * std)  # Limit extreme values\n",
    "X_reduced = (X_reduced - X_reduced.min()) / (X_reduced.max() - X_reduced.min()) * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a15afd1-9a2b-42c0-b0a3-e4bf34da11d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-max normalization to range 0-255\n",
    "arr_min = X_reduced.min()\n",
    "arr_max = X_reduced.max()\n",
    "rescaled_arr = (X_reduced - arr_min) / (arr_max - arr_min) * 255\n",
    "\n",
    "# Min-max normalization to range 0-255\n",
    "#arr_min = X_subset2_reduced.min()\n",
    "#arr_max = X_subset2_reduced.max()\n",
    "#rescaled_arr2 = (X_subset2_reduced - arr_min) / (arr_max - arr_min) * 255\n",
    "\n",
    "# Min-max normalization to range 0-255\n",
    "#arr_min = X_subset3_reduced.min()\n",
    "#arr_max = X_subset3_reduced.max()\n",
    "#rescaled_arr3 = (X_subset3_reduced - arr_min) / (arr_max - arr_min) * 255\n",
    "\n",
    "#X_combined = np.dstack([rescaled_arr1, rescaled_arr2, rescaled_arr3])\n",
    "#X_combined = X_combined.reshape(20000, 29700)  # Flatten for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6cffd734-f079-4be9-9b3f-342f24a7134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_grayscale_prenormalized_rescaled2_d=500_alpha=.01_lambda=10.npy', X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01cd5d3-a8e7-400b-b5f1-1e016f671ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigh(VVT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cabf84-5fe9-4a87-83e1-8e5744480071",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
